{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3bb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from operator import add\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pandas import Series,DataFrame\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7209eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArray(listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros(20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    \n",
    "    mysum = np.sum(returnVal)\n",
    "    \n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "    \n",
    "    return returnVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c0ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPagesFile=\"./data/SmallTrainingData.txt\"\n",
    "d_corpus = sc.textFile(wikiPagesFile, 1)\n",
    "d_keyAndText = d_corpus.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "d_keyAndListOfWords = d_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "allWordsWithDocID = d_keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = d_keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "\n",
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords = allCounts.top(20000, lambda x: x[1])\n",
    "topWords\n",
    "\n",
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK = sc.parallelize(range(20000))\n",
    "\n",
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
    "# (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = dictionary.join(allWordsWithDocID)\n",
    "\n",
    "# allDictionaryWords.take(1)\n",
    "\n",
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cad4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1 -- frequency position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66c268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The frequency position of the words 'applicant' is:  346\n",
      "The frequency position of the words 'and' is:  2\n",
      "The frequency position of the words 'attack' is:  502\n",
      "The frequency position of the words 'protein' is:  3014\n",
      "The frequency position of the words 'car' is:  608\n"
     ]
    }
   ],
   "source": [
    "# Frequency position:\n",
    "print(\"The frequency position of the words 'applicant' is: \", dictionary.filter(lambda x: x[0] == 'applicant').map(lambda x: x[1]).first())\n",
    "print(\"The frequency position of the words 'and' is: \", dictionary.filter(lambda x: x[0] == 'and').map(lambda x: x[1]).first())\n",
    "print(\"The frequency position of the words 'attack' is: \", dictionary.filter(lambda x: x[0] == 'attack').map(lambda x: x[1]).first())\n",
    "print(\"The frequency position of the words 'protein' is: \", dictionary.filter(lambda x: x[0] == 'protein').map(lambda x: x[1]).first())\n",
    "print(\"The frequency position of the words 'car' is: \", dictionary.filter(lambda x: x[0] == 'car').map(lambda x: x[1]).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "815ead1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2 learning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8500058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_X = allDocsAsNumpyArrays.map(lambda x: np.array(x[1]))\n",
    "# # rdd_X.take(2)                                 \n",
    "# rdd_y = allDocsAsNumpyArrays.map(lambda x: np.where(x[0][:2] == 'AU', 1, 0).tolist())   \n",
    "# # rdd_y.take(2)\n",
    "# traindata = rdd_y.zip(rdd_X)\n",
    "# testdata = rdd_y.zip(rdd_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d0abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindata.cache()\n",
    "# train_size = traindata.count()\n",
    "# # train_size # 3442"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab101762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_size = len(traindata.take(1)[0][1]) + 1\n",
    "# # parameter_size # 20001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a4e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def LogisticRegression(traindata=traindata,\n",
    "#                        max_iteration = 100,\n",
    "#                        learningRate = 0.01,\n",
    "#                        regularization = 0.01,\n",
    "#                        precision = 0.01,\n",
    "#                        optimizer = 'SGD' \n",
    "#                       ):\n",
    "\n",
    "#     # initialization\n",
    "#     prev_cost = 0\n",
    "#     L_cost = []\n",
    "#     prev_validation = 0\n",
    "#     train_size = traindata.count()\n",
    "\n",
    "#     parameter_size = len(traindata.take(1)[0][1]) + 1\n",
    "#     parameter_vector = np.zeros(parameter_size)\n",
    "# #     parameter_vector = np.zeros(parameter_size) # initialize with zeros\n",
    "#     momentum = np.zeros(parameter_size)\n",
    "#     prev_mom = np.zeros(parameter_size)\n",
    "#     second_mom = np.array(parameter_size)\n",
    "#     gti = np.zeros(parameter_size)\n",
    "#     epsilon = 10e-8\n",
    "    \n",
    "#     for i in range(max_iteration):\n",
    "\n",
    "#         bc_weights = parameter_vector[:-1]\n",
    "#         bc1_weights = parameter_vector[-1]\n",
    "\n",
    "\n",
    "#         res = traindata.treeAggregate((np.zeros(parameter_size), 0, 0),\\\n",
    "#               lambda x, y:(x[0]\\\n",
    "#                           + (np.append(y[1], 1)) * (-y[0] + (np.exp(np.dot(y[1], bc_weights) + bc1_weights)\\\n",
    "#                           /(1 + np.exp(np.dot(y[1], bc_weights) + bc1_weights)))),\\\n",
    "#                           x[1] \\\n",
    "#                           + y[0] * (-(np.dot(y[1], bc_weights) + bc1_weights)) \\\n",
    "#                           + np.log(1 + np.exp(np.dot(y[1],bc_weights)+ bc1_weights)),\\\n",
    "#                           x[2] + 1),\n",
    "#               lambda x, y:(x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "\n",
    "#         cost =  res[1]\n",
    "\n",
    "#         # calculate gradients\n",
    "#         gradient_derivative = (1.0 / res[2]) * res[0]\n",
    "        \n",
    "#         if optimizer == 'SGD':\n",
    "#             parameter_vector = parameter_vector - learningRate * gradient_derivative\n",
    "\n",
    "\n",
    "#         print(\"Iteration No.\", i, \" Cost=\", cost)\n",
    "        \n",
    "#         # Stop if the cost is not descreasing\n",
    "#         if abs(cost - prev_cost) < precision:\n",
    "#             print(\"cost - prev_cost: \" + str(cost - prev_cost))\n",
    "#             break\n",
    "#         prev_cost = cost\n",
    "#         L_cost.append(cost)\n",
    "        \n",
    "#     return parameter_vector, L_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_vector_sgd, L_cost_sgd = LogisticRegression(traindata=traindata,\n",
    "#                        max_iteration = 100,\n",
    "#                        learningRate = 0.8,\n",
    "#                        precision = 0.01,\n",
    "#                        optimizer = 'SGD' \n",
    "#                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ad101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b36617",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = allDocsAsNumpyArrays.map(lambda x: (np.array(x[1]), np.where(x[0][:2] == 'AU', 1, 0)))\n",
    "# myRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35bf8018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  Cost 2385.812595487336\n",
      "1  Cost 636.7181733349906\n",
      "2  Cost 524.7561613504523\n",
      "3  Cost 477.25189947925145\n",
      "4  Cost 451.6325149380995\n",
      "5  Cost 435.9799565004001\n",
      "6  Cost 425.6190121431654\n",
      "7  Cost 418.35135615101484\n",
      "8  Cost 413.01610374106383\n",
      "9  Cost 408.94823062661834\n",
      "10  Cost 405.7430454590628\n",
      "11  Cost 403.14252414854786\n",
      "12  Cost 400.9759489530843\n",
      "13  Cost 399.12692041996723\n",
      "14  Cost 397.5140801670866\n",
      "15  Cost 396.0793681051971\n",
      "16  Cost 394.7806152453219\n",
      "17  Cost 393.58673038569407\n",
      "18  Cost 392.4744912586729\n",
      "19  Cost 391.4263571933464\n",
      "20  Cost 390.42894877041476\n",
      "21  Cost 389.4719727781179\n",
      "22  Cost 388.5474503854033\n",
      "23  Cost 387.6491554501056\n",
      "24  Cost 386.7722007711947\n",
      "25  Cost 385.9127299908655\n",
      "26  Cost 385.0676859184938\n",
      "27  Cost 384.2346347815908\n",
      "28  Cost 383.4116318403595\n",
      "29  Cost 382.5971178907503\n",
      "30  Cost 381.7898390370957\n",
      "31  Cost 380.98878413577387\n",
      "32  Cost 380.19313575703353\n",
      "33  Cost 379.4022315575973\n",
      "34  Cost 378.61553372026447\n",
      "35  Cost 377.8326046795546\n",
      "36  Cost 377.05308777083695\n",
      "37  Cost 376.2766917538977\n",
      "38  Cost 375.50317839854904\n",
      "39  Cost 374.73235249975687\n",
      "40  Cost 373.96405382735554\n",
      "41  Cost 373.1981506213095\n",
      "42  Cost 372.43453432540673\n",
      "43  Cost 371.6731153160125\n",
      "44  Cost 370.91381943232597\n",
      "45  Cost 370.1565851536843\n",
      "46  Cost 369.40136130031124\n",
      "47  Cost 368.6481051582873\n",
      "48  Cost 367.89678094892395\n",
      "49  Cost 367.14735857815964\n",
      "50  Cost 366.39981261393933\n",
      "51  Cost 365.6541214494489\n",
      "52  Cost 364.9102666180199\n",
      "53  Cost 364.1682322319278\n",
      "54  Cost 363.42800452249577\n",
      "55  Cost 362.6895714630787\n",
      "56  Cost 361.9529224599143\n",
      "57  Cost 361.2180480985764\n",
      "58  Cost 360.4849399359979\n",
      "59  Cost 359.7535903298683\n",
      "60  Cost 359.02399229868433\n",
      "61  Cost 358.2961394069554\n",
      "62  Cost 357.5700256710529\n",
      "63  Cost 356.8456454820032\n",
      "64  Cost 356.1229935421897\n",
      "65  Cost 355.40206481347184\n",
      "66  Cost 354.6828544746775\n",
      "67  Cost 353.96535788678614\n",
      "68  Cost 353.2495705644214\n",
      "69  Cost 352.53548815252515\n",
      "70  Cost 351.82310640727553\n",
      "71  Cost 351.11242118048574\n",
      "72  Cost 350.4034284068563\n",
      "73  Cost 349.6961240935606\n",
      "74  Cost 348.99050431174595\n",
      "75  Cost 348.2865651895931\n",
      "76  Cost 347.5843029066561\n",
      "77  Cost 346.8837136892441\n",
      "78  Cost 346.1847938066521\n",
      "79  Cost 345.48753956808474\n",
      "80  Cost 344.7919473201435\n",
      "81  Cost 344.09801344476944\n",
      "82  Cost 343.40573435755726\n",
      "83  Cost 342.71510650636833\n",
      "84  Cost 342.0261263701853\n",
      "85  Cost 341.3387904581616\n",
      "86  Cost 340.6530953088252\n",
      "87  Cost 339.9690374894089\n",
      "88  Cost 339.2866135952771\n",
      "89  Cost 338.6058202494338\n",
      "90  Cost 337.92665410208946\n",
      "91  Cost 337.24911183027734\n",
      "92  Cost 336.57319013750725\n",
      "93  Cost 335.89888575344685\n",
      "94  Cost 335.2261954336253\n",
      "95  Cost 334.5551159591519\n",
      "96  Cost 333.88564413644724\n",
      "97  Cost 333.21777679698107\n",
      "98  Cost 332.55151079701716\n",
      "99  Cost 331.8868430173589\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-30-8664cec7fdbc>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-8664cec7fdbc>\"\u001b[1;36m, line \u001b[1;32m43\u001b[0m\n\u001b[1;33m    return cost_list\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Without Regularization\n",
    "learningRate = 0.001\n",
    "num_iteration = 100 \n",
    "\n",
    "myRDD.cache()\n",
    "\n",
    "allDocsAsNumpyArrays.cache()\n",
    "\n",
    "precision = 0.01\n",
    "\n",
    "oldCost = 0\n",
    "\n",
    "size = allDocsAsNumpyArrays.count()\n",
    "\n",
    "beta = np.zeros(20000)\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(num_iteration):\n",
    "    # first map : theta, x, y, e^theta\n",
    "    gradientCost = myRDD.map(lambda x : ((np.dot(x[0], beta)), x[0], x[1], np.exp(np.dot(x[0], beta))))\\\n",
    "                    .map(lambda x : ((-x[1]) * (x[2] - (x[3]/(1 + x[3]))), (- x[2] * x[0] + np.log(1 + x[3]))))\\\n",
    "                    .reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "    cost = gradientCost[1]\n",
    "    \n",
    "    gradient = gradientCost[0]\n",
    "    \n",
    "    cost_list.append(cost)\n",
    "        \n",
    "    print(i, \" Cost\", cost)\n",
    "    beta = beta - learningRate * gradient\n",
    "    \n",
    "    # Stop if the cost is not descreasing \n",
    "    if(abs(cost - oldCost) <= precision):\n",
    "        print(\"Stoped at iteration\", i)\n",
    "        break\n",
    "        \n",
    "    oldCost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32541a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No. 0  Cost= 2385.812595487336\n",
      "Iteration No. 10  Cost= 2096.2079361716783\n",
      "Iteration No. 20  Cost= 1861.1496601474453\n",
      "Iteration No. 30  Cost= 1669.402351331815\n",
      "Iteration No. 40  Cost= 1511.8788883596835\n",
      "Iteration No. 50  Cost= 1381.4133840652987\n",
      "Iteration No. 60  Cost= 1272.4278668836914\n",
      "Iteration No. 70  Cost= 1180.601398037059\n",
      "Iteration No. 80  Cost= 1102.585362867379\n",
      "Iteration No. 90  Cost= 1035.7748877365136\n",
      "Iteration No. 100  Cost= 978.1318587268526\n",
      "Iteration No. 110  Cost= 928.0504366757368\n",
      "Iteration No. 120  Cost= 884.2557334103205\n",
      "Iteration No. 130  Cost= 845.7276504882192\n",
      "Iteration No. 140  Cost= 811.6435443992552\n",
      "Iteration No. 150  Cost= 781.3348942344497\n",
      "Iteration No. 160  Cost= 754.2543738455296\n",
      "Iteration No. 170  Cost= 729.9506716678634\n",
      "Iteration No. 180  Cost= 708.0491036111616\n",
      "Iteration No. 190  Cost= 688.2365807535407\n",
      "Iteration No. 200  Cost= 670.2498706611949\n",
      "Iteration No. 210  Cost= 653.8663660213707\n",
      "Iteration No. 220  Cost= 638.8967748602693\n",
      "Iteration No. 230  Cost= 625.1792934518425\n",
      "Iteration No. 240  Cost= 612.5749309853016\n",
      "Iteration No. 250  Cost= 600.9637348550478\n",
      "Iteration No. 260  Cost= 590.2417247536453\n",
      "Iteration No. 270  Cost= 580.3183881085757\n",
      "Iteration No. 280  Cost= 571.1146227834702\n",
      "Iteration No. 290  Cost= 562.5610382386144\n",
      "Iteration No. 300  Cost= 554.5965456007854\n",
      "Iteration No. 310  Cost= 547.1671818519302\n",
      "Iteration No. 320  Cost= 540.2251247274094\n",
      "Iteration No. 330  Cost= 533.7278637421133\n",
      "Iteration No. 340  Cost= 527.6374996486988\n",
      "Iteration No. 350  Cost= 521.9201500332987\n",
      "Iteration No. 360  Cost= 516.5454430131874\n",
      "Iteration No. 370  Cost= 511.4860843769553\n",
      "Iteration No. 380  Cost= 506.7174861972492\n",
      "Iteration No. 390  Cost= 502.2174470991012\n"
     ]
    }
   ],
   "source": [
    "# Regularization test\n",
    "learningRate = 0.001\n",
    "num_iteration = 400 \n",
    "\n",
    "myRDD.cache()\n",
    "\n",
    "allDocsAsNumpyArrays.cache()\n",
    "\n",
    "precision = 0.05\n",
    "\n",
    "oldCost = 0\n",
    "\n",
    "size = allDocsAsNumpyArrays.count()\n",
    "\n",
    "beta = np.zeros(20000)\n",
    "\n",
    "cost_list_r = []\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(num_iteration):\n",
    "    # first map : theta, x, y, e^theta\n",
    "    gradientCost = myRDD.map(lambda x : ((np.dot(x[0], beta)), x[0], x[1], np.exp(np.dot(x[0], beta))))\\\n",
    "                    .map(lambda x : ((-x[1]) * (x[2] - (x[3]/(1 + x[3]))), (- x[2] * x[0] + np.log(1 + x[3]))))\\\n",
    "                    .reduce(lambda x, y : (x[0] + y[0], x[1] + y[1]))\n",
    "    \n",
    "    cost = gradientCost[1]\n",
    "    \n",
    "    # 0.001 as regulazition penalty\n",
    "    gradient = gradientCost[0] + 0.001 * np.linalg.norm(beta)\n",
    "    \n",
    "    cost_list_r.append(cost)\n",
    "        \n",
    "    if i%10 == 0 :\n",
    "            print(\"Iteration No.\", i, \" Cost=\", cost)\n",
    "            \n",
    "    beta = beta - learningRate * gradient\n",
    "    \n",
    "    # Stop if the cost is not descreasing       \n",
    "    if(abs(cost - oldCost) <= precision):\n",
    "        print(\"Stoped at iteration\", i)\n",
    "        break\n",
    "        \n",
    "    oldCost = cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb802c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('applicant', 346)]\n",
      "[('tribunal', 485)]\n",
      "[('respondent', 709)]\n",
      "[('mr', 206)]\n",
      "[('appellant', 1372)]\n"
     ]
    }
   ],
   "source": [
    "# Print out the five words with the largest regression coefficients\n",
    "d = beta.argsort()[-5:][::-1]\n",
    "for i in range(0, 5):\n",
    "    print(dictionary.filter(lambda x : (x[1] == d[i])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d10d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = beta.argsort()[-5:][::-1]\n",
    "for i in range(0, 5):\n",
    "    print(dictionary.filter(lambda x : (x[1] == d[i])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adfc79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 evaluation of learned model\n",
    "test = \"./data/TestingData.txt\"\n",
    "d_corpus_t = sc.textFile(test, 1)\n",
    "d_keyAndText_t = d_corpus_t.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "d_keyAndListOfWords_t = d_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "allWordsWithDocID_t = d_keyAndListOfWords_t.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords_t = d_keyAndListOfWords_t.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "\n",
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts_t = allWords_t.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords_t = allCounts_t.top(20000, lambda x: x[1])\n",
    "\n",
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK_t = sc.parallelize(range(20000))\n",
    "\n",
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
    "# (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary_t = topWordsK_t.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords_t = dictionary_t.join(allWordsWithDocID)\n",
    "\n",
    "# allDictionaryWords.take(1)\n",
    "\n",
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos_t = allDictionaryWords_t.map(lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc_t = justDocAndPos_t.groupByKey()\n",
    "\n",
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "allDocsAsNumpyArrays_t = allDictionaryWordsInEachDoc_t.map(lambda x: (x[0], buildArray(x[1])))\n",
    "# TF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
