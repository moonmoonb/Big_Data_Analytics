{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths on your local machine\n",
    "# Change this line later on your python script when you want to run this on the CLOUD (GC or AWS)\n",
    "\n",
    "# wikiPagesFile=\"/home/kia/wikiextractor/WikiPagesOutput/WikipediaPages_oneDocPerLine_1000Lines_small.txt\"\n",
    "# wikiCategoryFile=\"/home/kia/wikiextractor/wiki-categorylinks.csv.bz2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiPagesFile=\"./data/WikipediaPagesOneDocPerLine1000LinesSmall.txt.bz2\"\n",
    "wikiCategoryFile=\"./data/wiki-categorylinks-small.csv.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"434042\",\"1987_debut_albums\"', '\"434042\",\"Albums_produced_by_Mike_Varney\"']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read two files into RDDs\n",
    "\n",
    "wikiCategoryLinks=sc.textFile(wikiCategoryFile)\n",
    "\n",
    "wikiCats=wikiCategoryLinks.map(lambda x: x.split(\",\")).map(lambda x: (x[0].replace('\"', ''), x[1].replace('\"', '') ))\n",
    "\n",
    "# Now the wikipages\n",
    "wikiPages = sc.textFile(wikiPagesFile)\n",
    "\n",
    "wikiCategoryLinks.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('434042', '1987_debut_albums')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiCats.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(wikiPagesFile)\n",
    "\n",
    "# Uncomment this line if you want to take look inside the file. \n",
    "# df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wikiPages.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Assumption: Each document is stored in one line of the text file\n",
    "# We need this count later ... \n",
    "numberOfDocs = wikiPages.count()\n",
    "\n",
    "print(numberOfDocs)\n",
    "# Each entry in validLines will be a line from the text file\n",
    "validLines = wikiPages.filter(lambda x : 'id' in x and 'url=' in x)\n",
    "\n",
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6])) \n",
    "\n",
    "# keyAndText.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildArray(listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros(20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    \n",
    "    mysum = np.sum(returnVal)\n",
    "    \n",
    "    returnVal = np.divide(returnVal, mysum)\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "\n",
    "def build_zero_one_array (listOfIndices):\n",
    "    \n",
    "    returnVal = np.zeros (20000)\n",
    "    \n",
    "    for index in listOfIndices:\n",
    "        if returnVal[index] == 0: returnVal[index] = 1\n",
    "    \n",
    "    return returnVal\n",
    "\n",
    "\n",
    "def stringVector(x):\n",
    "    returnVal = str(x[0])\n",
    "    for j in x[1]:\n",
    "        returnVal += ',' + str(j)\n",
    "    return returnVal\n",
    "\n",
    "\n",
    "\n",
    "def cousinSim (x,y):\n",
    "\tnormA = np.linalg.norm(x)\n",
    "\tnormB = np.linalg.norm(y)\n",
    "\treturn np.dot(x,y)/(normA*normB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words in Corpus: [('the', 74530), ('of', 34512), ('and', 28479), ('in', 27758), ('to', 22583), ('a', 21212), ('was', 12160), ('as', 8811), ('for', 8773), ('on', 8435)]\n",
      "Word Postions in our Feature Matrix. Last 20 words in 20k positions:  [('quebecor', 19999), ('poten', 19998), ('kasada', 19997), ('yadnya', 19996), ('drift', 19995), ('iata', 19994), ('satire', 19993), ('expreso', 19992), ('olimpico', 19991), ('auxiliaries', 19990), ('tenses', 19989), ('petherick', 19988), ('stowe', 19987), ('infimum', 19986), ('parramatta', 19985), ('rimpac', 19984), ('hyderabad', 19983), ('cubes', 19982), ('meats', 19981), ('chaat', 19980)]\n"
     ]
    }
   ],
   "source": [
    "# Now, we transform it into a set of (docID, text) pairs\n",
    "keyAndText = validLines.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "\n",
    "# Now, we split the text in each (docID, text) pair into a list of words\n",
    "# After this step, we have a data set with\n",
    "# (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# We use a regular expression here to make\n",
    "# sure that the program does not break down on some of the documents\n",
    "\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "# remove all non letter characters\n",
    "keyAndListOfWords = keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "# better solution here is to use NLTK tokenizer\n",
    "\n",
    "# Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = keyAndListOfWords.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "\n",
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x + y)\n",
    "\n",
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords = allCounts.top(20000, lambda x: x[1])\n",
    "topWords\n",
    "\n",
    "# \n",
    "print(\"Top Words in Corpus:\", allCounts.top(10, key=lambda x: x[1]))\n",
    "\n",
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "topWordsK = sc.parallelize(range(20000))\n",
    "\n",
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 1)\n",
    "# (\"NextMostCommon\", 2), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "\n",
    "print(\"Word Postions in our Feature Matrix. Last 20 words in 20k positions: \", dictionary.top(20, lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allDocsAsNumpyArrays.take(3):\n",
      "\n",
      "[('432039', array([0.07925072, 0.03775216, 0.04351585, ..., 0.        , 0.        ,\n",
      "       0.        ])), ('432044', array([0.04477612, 0.0261194 , 0.02985075, ..., 0.        , 0.        ,\n",
      "       0.        ])), ('432114', array([0.0815047 , 0.05956113, 0.02821317, ..., 0.        , 0.        ,\n",
      "       0.        ]))]\n"
     ]
    }
   ],
   "source": [
    "################### TASK 2  ##################\n",
    "\n",
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "\n",
    "allWordsWithDocID = keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "\n",
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = dictionary.join(allWordsWithDocID)\n",
    "\n",
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map(lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "print('allDocsAsNumpyArrays.take(3):')\n",
    "print()\n",
    "print(allDocsAsNumpyArrays.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allDocsAsNumpyArraysTFidf.take(2):\n",
      "\n",
      "[('432039', array([0.00669424, 0.00314784, 0.00611015, ..., 0.        , 0.        ,\n",
      "       0.        ])), ('432044', array([0.0037822 , 0.00217788, 0.00419141, ..., 0.        , 0.        ,\n",
      "       0.        ]))]\n"
     ]
    }
   ],
   "source": [
    "# Now, create a version of allDocsAsNumpyArrays where, in the array,\n",
    "# every entry is either zero or one.\n",
    "# A zero means that the word does not occur,\n",
    "# and a one means that it does.\n",
    "\n",
    "zeroOrOne = allDocsAsNumpyArrays.mapValues(lambda x: np.where(np.array(x) == 0, 0, 1) )\n",
    "\n",
    "# Now, add up all of those arrays into a single array, where the\n",
    "# i^th entry tells us how many\n",
    "# individual documents the i^th word in the dictionary appeared in\n",
    "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
    "\n",
    "# Create an array of 20,000 entries, each entry with the value numberOfDocs (number of docs)\n",
    "multiplier = np.full(20000, numberOfDocs)\n",
    "\n",
    "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\n",
    "# i^th word in the corpus\n",
    "idfArray = np.log(np.divide(np.full(20000, numberOfDocs), dfArray))\n",
    "\n",
    "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors\n",
    "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
    "\n",
    "\n",
    "# use the buildArray function to build the feature array\n",
    "# allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], buildArray(x[1])))\n",
    "\n",
    "print('allDocsAsNumpyArraysTFidf.take(2):')\n",
    "print()\n",
    "print(allDocsAsNumpyArraysTFidf.take(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('434042', '1987_debut_albums')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiCats.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Asteroid_spectral_classes',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('S-type_asteroids',\n",
       "  array([0.00674469, 0.00348744, 0.00373721, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('All_article_disambiguation_pages', array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " ('All_disambiguation_pages', array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " ('Disambiguation_pages_with_short_description',\n",
       "  array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " ('Human_name_disambiguation_pages', array([0., 0., 0., ..., 0., 0., 0.])),\n",
       " ('1780_births',\n",
       "  array([0.00676836, 0.00400873, 0.00420036, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('1845_deaths',\n",
       "  array([0.00676836, 0.00400873, 0.00420036, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('Argentine_monarchists',\n",
       "  array([0.00676836, 0.00400873, 0.00420036, ..., 0.        , 0.        ,\n",
       "         0.        ])),\n",
       " ('Argentine_people_of_Spanish_descent',\n",
       "  array([0.00676836, 0.00400873, 0.00420036, ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we join it with categories, and map it after join so that we have only the wikipageID \n",
    "# This joun can take time on your laptop. \n",
    "# You can do the join once and generate a new wikiCats data and store it. Our WikiCategories includes all categories\n",
    "# of wikipedia. \n",
    "\n",
    "featuresRDD = wikiCats.join(allDocsAsNumpyArraysTFidf).map(lambda x: (x[1][0], x[1][1]))\n",
    "\n",
    "# Cache this important data because we need to run kNN on this data set. \n",
    "featuresRDD.cache()\n",
    "featuresRDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13780"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us count and see how large is this data set. \n",
    "featuresRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally, we have a function that returns the prediction for the label of a string, using a kNN algorithm\n",
    "def getPrediction (textInput, k):\n",
    "    # Create an RDD out of the textIput\n",
    "    myDoc = sc.parallelize (('', textInput))\n",
    "\n",
    "    # Flat map the text to (word, 1) pair for each word in the doc\n",
    "    wordsInThatDoc = myDoc.flatMap (lambda x : ((j, 1) for j in regex.sub(' ', x).lower().split()))\n",
    "\n",
    "    # This will give us a set of (word, (dictionaryPos, 1)) pairs\n",
    "    allDictionaryWordsInThatDoc = dictionary.join (wordsInThatDoc).map (lambda x: (x[1][1], x[1][0])).groupByKey ()\n",
    "\n",
    "    # Get tf array for the input string\n",
    "    myArray = buildArray (allDictionaryWordsInThatDoc.top (1)[0][1])\n",
    "    \n",
    "    # Get the tf * idf array for the input string\n",
    "    myArray = np.multiply(myArray, np.log(np.divide(np.full(20000, numberOfDocs), dfArray)))\n",
    "\n",
    "    # Get the distance from the input text string to all database documents, using cosine similarity (np.dot() )\n",
    "    distances = featuresRDD.map (lambda x : (x[0], np.dot (x[1], myArray)))\n",
    "    # distances = allDocsAsNumpyArraysTFidf.map (lambda x : (x[0], cousinSim (x[1],myArray)))\n",
    "    # get the top k distances\n",
    "    topK = distances.top (k, lambda x : x[1])\n",
    "    \n",
    "    # and transform the top k distances into a set of (docID, 1) pairs\n",
    "    docIDRepresented = sc.parallelize(topK).map (lambda x : (x[0], 1))\n",
    "\n",
    "    # now, for each docID, get the count of the number of times this document ID appeared in the top k\n",
    "    numTimes = docIDRepresented.reduceByKey(lambda x,y : x + y)\n",
    "    \n",
    "    # Return the top 1 of them.\n",
    "    # Ask yourself: Why we are using twice top() operation here?\n",
    "    return numTimes.top(k, lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Human_name_disambiguation_pages', 1), ('All_disambiguation_pages', 1), ('Lists_of_sportspeople_by_sport', 1), ('Disambiguation_pages_with_short_description', 1), ('Bullfighters', 1), ('All_articles_with_dead_external_links', 1), ('All_article_disambiguation_pages', 1), ('2015_deaths', 1), (\"Air_Force_Falcons_men's_basketball_coaches\", 1), ('1931_births', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('Sport Basketball Volleyball Soccer', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All_set_index_articles', 2), ('Articles_with_short_description', 2), ('All_Wikipedia_articles_written_in_Australian_English', 2), ('Use_Australian_English_from_April_2018', 1), ('Royal_Australian_Navy_ship_names', 1), ('Use_dmy_dates_from_April_2018', 1), ('Set_indices_on_ships', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('What is the capital city of Australia?', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All_stub_articles', 2), ('CBC_Television_shows', 1), ('1979_births', 1), ('1990s_Canadian_teen_drama_television_series', 1), ('1991_Canadian_television_series_debuts', 1), ('Canadian_television_program_stubs', 1), ('1994_Canadian_television_series_endings', 1), ('Television_shows_set_in_Vancouver', 1), ('Ak_Bars_Kazan_players', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(getPrediction('How many goals Vancouver score last year?', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Congradulations, you have implemented a prediction system based on Wikipedia data. \n",
    "# You can use this system to generate automated Tags or Categories for any kind of text \n",
    "# that you put in your query.\n",
    "# This data model can predict categories for any input text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
